{"data":{"allMarkdownRemark":{"totalCount":13,"edges":[{"node":{"id":"551fb668-b448-5bf1-972a-72dead6d2b60","frontmatter":{"title":"B-L475E-IOT01A - Adding in sensor readings","date":"14 Feb, 2021"},"fields":{"slug":"/posts/stm32discovery_3/"},"excerpt":"I spent a long time trying to get the BLE functionality to work, but no success on that branch so far. So in order to avoid letting my ego…","internal":{"content":"\nI spent a long time trying to get the BLE functionality to work, but no success on that branch so far. So in order to avoid letting my ego roll into a pit of despair, I decided to take a break and do something much easier: add in all the sensor readings. \n\nBefore I could do that, I needed to create a function that lets me print floats over usb, by converting a `float` to a `char*`. Looking around online, the answer seemed to be using `snprintf`. However, I tried using this function to do just that but did not succeed. I then tried compiling this with clang, and I was able to get it working. Hmm! Turns out, not all compilers will support float with `snprintf`, as was the case here. So instead, I had to create a function from scratch myself. Probably not the nicest looking function, but here it is. Check out the repo for more details, should be there as of commit e9492087f8389e4e541dcfb5e9cae6c7b8aa36b1.\n\n```cpp\n#include \"sx_util.h\"\n\nvoid usbprint_float(float value, int precision)\n{\n    int integer_pt = value;\n    volatile float fraction = (value-integer_pt)*pow(10,precision);\n    int fraction_pt = fraction;\n\n    // count number of digits\n    volatile int count_int = 0;\n    volatile int count_fract = 0;\n    int n = integer_pt;\n    while (n != 0) {\n        n /= 10;  \n        ++count_int;\n    }\n    n = fraction_pt;\n    while (n != 0) {\n        n /= 10;  \n        ++count_fract;\n    }\n\n    // Convert integer and fraction parts to char buffers\n    char int_buf[16];                        // NOTE: MAX integer length of 16\n    char fract_buf[16];                      // NOTE: MAX decimal length of 16\n\n    snprintf(int_buf,10,\"%d\",integer_pt);\n    snprintf(fract_buf,10,\"%d\",fraction_pt);\n\n    // Add on the decimal point\n    strcat(int_buf,\".\");\n    // Add the newline\n    strcat(fract_buf,\"\\n\");\n\n    char outStr[16];\n    // Put the characters into the array\n    for(int i=0;i<count_int+1;i++){\n        outStr[i] = int_buf[i];\n    }\n    for(int i=0;i<=count_fract+1;i++){\n        outStr[count_int+1+i] = fract_buf[i];\n    }\n\n    usb_print((uint8_t*)outStr,count_int+count_fract+2);\n    \n}\n```\n\nFinally, all I had to do was create a Sensors class that initialized all the sensors on the constructor.\n```\nTemperature Reading: 25.1\nHumidity Reading: 57.27\nPressure Reading: 1013.70\nAccel X: 5.0, Y: 413.0, Z: 916.0\nGyro X: 210.0, Y: -14000, Z: 350.0\nMagneto X: 672.0, Y: -4200, Z: -4290\n```\n\nSensor readings all seem to come out fine, though I'm not totally sure what the units on the gyro, magneto are. I'll take a look another time. Time to get back to banging my head on the BLE first, QSPI next."}}},{"node":{"id":"48de7c32-6757-543d-a43c-254191e577d5","frontmatter":{"title":"B-L475E-IOT01A - Including Busy Wait on USB transmit","date":"03 Feb, 2021"},"fields":{"slug":"/posts/stm32discovery_2/"},"excerpt":"Since I'm starting off right now coding on bare metal (I'm not running an RTOS), I've got only the main() thread running and no other task…","internal":{"content":"\nSince I'm starting off right now coding on bare metal (I'm not running an RTOS), I've got only the main() thread running and no other task manager to speak of. This means I have limited options when addressing the flow of tasks in my code. For example, what I've observed is that if I make two calls to `CDC_Transmit_FS` consecutively, the second one doesn't run:\n\n```cpp\nvoid ExampleClass::printChar()\n{\n    int len = strlen(this->charValue);          // charValue = \"charstr\"\n    uint8_t charout[] = \"printChar Output: \";\n    CDC_Transmit_FS(charout, 18);\n    CDC_Transmit_FS((uint8_t*)this->charValue,len);        \n}\n```\n\nResults with a printout of the following\n```\n--- Miniterm on /dev/cu.usbmodem205A319838471  9600,8,N,1 ---\n--- Quit: Ctrl+C | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\nHello World!\nprintChar Output: Hello World!\nprintChar Output: Hello World!\nprintChar Output: Hello World!\nprintChar Output: Hello World!\n```\n\nI expected to see \"charstr\\n\" after printChar output (so charstr with a carriage return). Why does this happen? Because the call to `CDC_Transmit_FS` gets called while the USB peripheral is still busy. Therefore, the way to fix this is to include CDC_Transmit_FS with another function that will do a busy wait.\n\n```cpp\nvoid usb_print(uint8_t* Buf, uint16_t Len){\n    while (CDC_Transmit_FS(Buf,Len)) {}     // Busy wait to ensure that the transmit completes\n}\n\nvoid ExampleClass::printChar()\n{\n    int len = strlen(this->charValue);\n    uint8_t charout[] = \"printChar Output: \";     \n\n    usb_print(charout,18);\n    usb_print((uint8_t*)this->charValue,len);\n}\n```\n\nAnd here is the result!\n```\n--- Miniterm on /dev/cu.usbmodem205A319838471  9600,8,N,1 ---\n--- Quit: Ctrl+C | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\nHello World!\nprintChar Output: charstr\nHello World!\nprintChar Output: charstr\nHello World!\nprintChar Output: charstr\n```"}}},{"node":{"id":"139e6e09-be07-59eb-81b8-f5889bfa9c6e","frontmatter":{"title":"Getting started iwth the B-L475E-IOT01A STM32 Discovery kit (IoT Node)","date":"30 Jan, 2021"},"fields":{"slug":"/posts/stm32discovery_1/"},"excerpt":"Reader's note: These instructions are specifically for Mac OSX (I was running Catalina). I'm currently trying to port over everything onto…","internal":{"content":"\nReader's note: These instructions are specifically for Mac OSX (I was running Catalina). I'm currently trying to port over everything onto my linux computer, but I am running into compile issues there.\n\nSince I've decided it was time to actually learn how to write embedded code, I decided the best thing to do was to start with this free STM32 Discovery kit, which I got for free after attending a conference. It's neat because this board is packed to the brim with sensors, and also includes a WiFi and BLE chip. I only which it had a cell module too, but nothing's perfect.\n\nIt's easy to get started with this board because STM32 provides the STM32CubeMX application, which generates code for you automatically. The first thing I wanted to do was to get the USB port working so that I could write serial logs. This was pretty easy to do by following this setup here: https://www.youtube.com/watch?v=AYICE0gU-Sg. Of course, there were some minor differences because the board from that video was different from this one. You can see exactly what by opening the .ioc file in my repo https://github.com/simjxu/stm32_discovery (as of commit hash 1798e96db2c7153ad87412712790d49240252aee). \n\nFrom there, code was generated. I opted to use the VS Code plugin called \"PlatformIO\" to help build and read from terminal. The code generated from CubeMX actually had some compile errors when I tried to follow the tutorial. Turns out, it didn't like that a couple of the functions had a mismatched type in the last argument. It was expecting uint16_t, but you can see a uint32_t.\n\n```cpp\nUSBD_StatusTypeDef USBD_LL_Transmit(USBD_HandleTypeDef *pdev, uint8_t ep_addr,\n                                    uint8_t *pbuf, uint32_t size);\n\nUSBD_StatusTypeDef USBD_LL_PrepareReceive(USBD_HandleTypeDef *pdev, uint8_t ep_addr,\n                                          uint8_t *pbuf, uint32_t size);\n```\n\nI edited to say uint16_t, and compilations ended up happening successfully, but for some reason this switched back to uint32_t, but with no issues compiling after that point... So I'm not sure what's happening here. Nevertheless, I was able to start printing my helloworld over serial. Note that both USB connectors need to be connected in order to read off of that port (see picture).\n\nResults:\n![board_2xusb](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/BL475E_usb.jpg)\n\nAs a side note, you are also able to compile in C++ code, no problemo. I tried adding in a class, and was able to get it working and printing out (see classtest.h and classtest.cpp).\n\n```cpp\nclass FatherProperty {\nprivate:\n    static uint8_t propValue[];        // I need to declare this as static, why?\n    // static means that there is only one copy of propValue, no matter how many times the class is instantiated\n\npublic:\n    void getProperty();\n};\n\nuint8_t FatherProperty::propValue[]=\"asdf\\n\";\nvoid FatherProperty::getProperty() \n{\n    int len = sizeof(this->propValue);\n    CDC_Transmit_FS(this->propValue,len-1);         // subtract 1 so that it does not include the extra character\n}\n```\n"}}},{"node":{"id":"25b5043d-d610-5435-a47e-cef88779854b","frontmatter":{"title":"How to make a Syntax highlighted Notepad with Sublime","date":"08 Feb, 2019"},"fields":{"slug":"/posts/sublimehighlight/"},"excerpt":"To create a new syntax, go to Tools >> Developer >> New Syntax. This creates a .sublime-syntax file which you can use to edit what color the…","internal":{"content":"\nTo create a new syntax, go to Tools >> Developer >> New Syntax. This creates a .sublime-syntax file which you can use to edit what color the text will show as. Under the \"match:\" key value pair, you use regex to determine which types of text it matches. In the example shown below, I have dates with a newline character highlight as orange. The orange color for the Mariana theme matches the \"scope:\" key value pair of constant.numeric. What I essentially had done was look in the Mariana.sublime-color-scheme file to determine that this scope was an orange color.\n\nTypically, this file is found in Sublime Text 3 >> Packages >> User. On Mac, this is under <username>/Library/Application Support/Sublime Text 3/Packages\n\nTo edit the Mariana theme file, on Mac use CMD+Shift+P to open the command palette, type \"View Package File\", and then type in Mariana to edit the file Mariana.sublime-color-scheme.\n\nMy file is called Plain text.sublime-syntax\n\n\n```yaml\n%YAML 1.2\n---\n# http://www.sublimetext.com/docs/3/syntax.html\nname: Regular Text\nfile_extensions:\n  - txt\nscope: text.plain\ncontexts:\n  main:\n    # Line starting with **\n    - match: '^\\*\\*.*'\n      push:\n        - meta_scope: meta.separator\n        - match: '$\\n?'\n          pop: true\n    # Highlight Plan\n    - comment: Plan (case-insensitive)\n      match: \\b(?:Plan)\\b\n      scope: support.function\n    # Dates\n    - comment: Dates\n      match: '[0-9]{1,2}(/)[0-9]{1,2}(/)[0-9]{1,2}\\n'\n      scope: constant.numeric\n      pop: true\n    # Meeting\n    - comment: Meetings\n      match: '.*Meeting.*\\n'\n      scope: string\n      pop: true\n    # Meeting\n    - comment: Meetings\n      match: '.*mtg\\n'\n      scope: string\n      pop: \n    # Feedback\n    - comment: Feedback\n      match: 'Feedback.*\\n'\n      scope: variable.member\n      pop: true\n    # Important\n    - comment: Important\n      match: '.*:\\n'\n      scope: support.function\n      pop: true\n```\n\nResults:\n![sublime_pic](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/sublime-highlight.jpg)"}}},{"node":{"id":"634fb842-9758-5f92-9e2d-b197c7f6e5a4","frontmatter":{"title":"How to write into a Firebase Realtime Database using REST/Express","date":"27 Dec, 2018"},"fields":{"slug":"/posts/firebase-rest/"},"excerpt":"To start off, let's start with something simple, updating a Firebase database with the help of Firebase's Cloud Functions. What makes Cloud…","internal":{"content":"\nTo start off, let's start with something simple, updating a Firebase database with the help of Firebase's Cloud Functions. What makes Cloud Functions easy to use is that it is \"serverless\", or in other words you do not have to do any work to manage a server instance, making sure that it is running, that it isn't getting overloaded, etc. Instead, you are charged for function calls that are done. Google will manage everything else in their backend. By the way, download Postman if you don't already have it, it helps you test out your requests more easily. My project is located here: https://github.com/simjxu/firebase_firestore Here's what I will accomplish in this tutorial:\n\n1. Create an API endpoint to perform GET and POST requests to a Firebase Firestore database.\n2. Pull that database data into an HTML webpage\n3. Deploy all of that above and connect to a personal domain.\n\nSetup:\nOn console.firebase.google.com, you will want to login with your account and create a new project. The project I created is called sjx-restapi. Then, on the left panel of the webpage, you can click Database and Cloud Firestore to create a new Firestore database. Once your firestore database is all set up, then you can create a new folder where you want your REST API firestore app to reside, navigate to it on your terminal and use `firebase init`. (I assumed you already did an `npm install firebase`, check online for how to get the command line tools for firebase). It will ask you what features you would like, and you should include Firestore and Cloud Functions at minimum for this example. Then, connect it to your project. All other options you can just use the default. By doing a `firebase init`, you create a firebase.json file which allows you to test out this webapp and deploy it. \n\nCoding:\nI have 3 main files within my Firebase project folder: functions/index.js, public/index.html, public/app.js. \"index.js\" is what holds my REST API endpoints to handle incoming requests, through `curl` or Postman. \"index.html\" is a simple html page that will show the contents of a particular collection and document in my Firestore database. \"app.js\" is a script that is used to pull the data into the html page in realtime. Let's get started.\n\nFor index.js, you can run an express app on Cloud Functions that will allow you to handle GET/POST. You can handle other things as well, feel free to add it in. Express is a framework which allows you to create handlers. In this example, notice references to `db.collection('devices').doc('Device1')`; these refer to the structure in Cloud Firestore, where collections hold documents which hold collections which hold documents, and so on and so forth. I've created a collection called \"devices\", because this will hold all my IoT device data, and a document Device1 is currently what I am using to test this endpoint out.\n\n```javascript\n// Use admin to access the database, functions to run this app\n// as a Cloud function, and Express to handle REST requests\nconst admin = require('firebase-admin');\nconst functions = require('firebase-functions');\nconst express = require('express');\nconst app = express();\n\n// Initialize the database\nadmin.initializeApp(functions.config().firebase);\nvar db = admin.firestore();\n\n// ------ API GET/POST request handlers --------------------------------------\napp.get('/database', (req, res) => {\n  var Device1Ref = db.collection('devices').doc('Device1');\n  var getDoc = Device1Ref.get().then(doc => {\n    res.send(doc.data());\n    return 0;\n  }).catch(err => {\n    console.log(\"Error: \", err);\n    res.send(\"Error\");\n    return 0;\n  });\n});\n\napp.post('/database', (req, res) => {\n  res.send('POST sent');\n\n  // Now add all these to the database\n  var Device1Ref = db.collection('devices').doc('Device1');\n  req.body.forEach(element => {\n    Device1Ref.set(element);\n\n  });\n});\n\n// Run App on Cloud Functions\nexports.app = functions.https.onRequest(app);\n```\n\nI'm not going to spend much time on the index.html file, but you can take a look at the file on Github. It runs a script \"app.js\", which has my config data for my Firestore database. Again, I look in the \"devices\" collection and pull the document \"Device1\". From here, I just create a realtime updating function which pulls the stringified JSON from the Firestore Database when changes are made.\n\n```javascript\n// Initialize Firebase\nvar config = {\n  ***config***\n};\nfirebase.initializeApp(config);\nvar firestore = firebase.firestore();\n\n// Define the collection and document to pull\nconst docRef = firestore.collection(\"devices\").doc(\"Device1\");\n\n// Identify the HTML tag id to output data to\nconst outputString = document.querySelector(\"#deviceData\");\n\n// Pull the json from the collection/document in firestore and \n// update automatically\ngetRealtimeUpdates = function() {\n  docRef.onSnapshot(function (doc) {\n    if (doc && doc.exists) {\n      const myData = doc.data();\n      var myStr = JSON.stringify(myData, null, 2);\n      outputString.innerText = myStr;\n    }\n  });\n}\ngetRealtimeUpdates();\n```\n\nOnce this is all set, you can use `firebase serve` to run a localhost server to test out your GET/POST calls using Postman. The only notes here are that I set my Headers to `Content-Type: application/json`, and my body to be `raw JSON(application/json)`. Here is an example of something I put in a body:\n```json\n[\n\t{\n\t\t\"key1\": \"testbodyvalA\",\n\t\t\"key2\": \"testbodyvalB\",\n\t\t\"key3\": \"testbodyvalC\",\n\t\t\"key4\": \"testbodyvalD\",\n\t\t\"key5\": \"testbodyvalE\"\n\t}\n]\n```\n\n\n\n"}}},{"node":{"id":"c04a8650-82f7-5596-9ca9-323146f20280","frontmatter":{"title":"New Project Plan","date":"18 Dec, 2018"},"fields":{"slug":"/posts/projectplan1218/"},"excerpt":"I spent a couple weekends looking into Kubernetes, and it looks to be start of a new project. So this post will outline the plans and…","internal":{"content":"\nI spent a couple weekends looking into Kubernetes, and it looks to be start of a new project. So this post will outline the plans and requirements for the different modularized microservices that I will make as part of my project, in app.simonxu.com.\n\n*Microservice 1: Database Display*\nThe first one will be the most basic, it will update to show the contents of a firebase datastore including key-value pairs that are in my firebase database. This will update automatically \n\n*Microservice 2: REST API*\nThis will be some basic GET, POST, PUT, DELETE actions from REST based communication. I should be able to use the API I create to make requests vias Postman.\n\n*Microservice 3: GraphQL API*\nThis one will be a little fancier, using GraphQL to pick up specific items in my database. Creating an API that uses GraphQL will help me understand it better.\n\nTo make actions using the API, I will also need to set up a token-based authentication system using OAuth2, and in order to test it out, I will make a time based trigger which will update a \"date\" key with the current date in UTC once day. Probably in the beginning I will avoid having to do refreshes on the token, but once I understand it more I will add some code that can automatically do the token refresh.\n\nAll of this will be orchestrated using Kubernetes.\n\nIf there is time:\n- machine learning stuff"}}},{"node":{"id":"5002ed5e-b35d-5579-a78e-43e0e064d7b3","frontmatter":{"title":"How to use Google Sheets as a Post/Get/Webhook Endpoint","date":"11 Nov, 2018"},"fields":{"slug":"/posts/particlepostgetwebhk/"},"excerpt":"Shameful plug post, but I'll be using Particle devices to demonstrate how to setup a Post/Get/Webhook right on a Google Sheet. Although…","internal":{"content":"\nShameful plug post, but I'll be using Particle devices to demonstrate how to setup a Post/Get/Webhook right on a Google Sheet. Although Excel has more spreadsheet features than Google Sheets, the nice thing is that since your google sheets resides on some server connected to the internet, you can essentially use Google Sheets as a basic server to receive and send requests to an IoT device somewhere else. Take a look at how simple this spreadsheet is:\n\n![googsht-GPW](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/googlesht-getpostwebhk.jpg)\n\nAll requests will operate under a request/response format, the idea being that the Google Sheet I made will shoot off a request, and a response will be collected. ONE IMPORTANT NOTE: Since you will be allowing anonymous access in order to use Google Apps Script, PLEASE only use this for development testing purposes ONLY, not to run a production server. If someone finds your Google Sheets URL endpoint, they can flood your sheet with requests. Furthermore, your device access token may be exposed.\n\nWith that said, let's get started.\n\nFirst you will want to create a new Google sheet which looks like what I've shown above. You can ignore cells B3, C3, and D2-D3, as these will be populated automatically by the webapp you'll create using Google Apps Script. You don't have to do this, but I merged cell D2 and D3. To create the buttons, go to Insert >> Drawing to draw a basic rectangle with some text in the middle. You will be able to assign function to these buttons you created later.\n\nParticle (www.particle.io) makes it easy for you connect sensors to the internet. I'm using a Particle Photon in this example, but you can use whatever you want. Take a look at docs.particle.io to get your Photon up and running. From the starter kit, you can set up your resistors, LED, and photoresistor like I did in the below image:\n\n![photon-pic](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/particle-photon.JPG)\n\nYou will need to flash a binary to the Photon that sets it up to receive function calls and publish variables and events. You can how I did that in this script: https://github.com/simjxu/partiscripts/blob/master/getpostwebhook-example/photoresistor.ino \n\nIn my `setup()` section I've called out 3 different `Particle.variables`: `analogvalue`, which is the output from the photoresistor, `brightness`, which is the brightness of the LED, and `setpoint`, which is the setpoint issued to the LED. I've also created a `Particle.function` called `led`, which toggles the blue LED situated on D7 on the board. Lastly, I've created a `Particle.publish`, which tracks the analog value and publishes a `low_light` event when the value drops below 50. \n\nWhat I will do now is use my \"Post Request\" function on my google sheet to call the `led` function and toggle on/off the LED. I will use the \"Get Request\" button to pull the `analogvalue` variable. Lastly, I will create a webhook to publish the `low_light` event details to my Google sheet when the event publishes.\n\nFirst, go to the Google Sheet and go to Tools >> Script Editor. This will allow you write javascript which controls how your Google Sheet behaves. Start off in the script by setting some Global variables. You have to retrieve you access token from build.particle.io, by clicking the settings tab. The device id is the device of your photon, and spreadsheet id is the long string of in your Google Sheets URL. You can also edit the Sheet name if you decide to change that.\n\n```javascript\n// Set Global Variables \n/* \nACCESS_TOKEN: Go to build.particle.io and click \"Settings\" on the left side panel\nPARTICLE_DEVICE_ID: Identify device ID from the console, or from using \"particle list\"\nSPREADSHEET_ID: Look on the URL for your Google Sheet for the long random string\n*/\nPropertiesService.getScriptProperties().setProperty('SHEET_NAME', 'Sheet1');\nPropertiesService.getScriptProperties().setProperty('PARTICLE_ACCESS_TOKEN', '<enter access token>');\nPropertiesService.getScriptProperties().setProperty('PARTICLE_DEVICE_ID', '<enter device id>');\nPropertiesService.getScriptProperties().setProperty('SPREADSHEET_ID', '<enter spreadsheetid>');\n```\n\nHandling the Post Request:\nGo back to your Google Sheet, and right click the button you labeled \"MAKE POST REQUEST\", and assign it a function called `PostRequest` or whatever you decide to name it. In the script file, add the function below. You will be using the Particle API URL, and sending the request there. The request will require your access token, and the argument (in the case above, either \"on\" or \"off\" for the LED).\n\n```javascript\n// Handle the button click for Post Request -------------------------------------------------\nfunction PostRequest() {\n  var spreadsheet_id = SpreadsheetApp.getActiveSpreadsheet().getId();\n  var sheet = SpreadsheetApp.openById(spreadsheet_id).getSheetByName( \n    PropertiesService.getScriptProperties().getProperty('SHEET_NAME'));\n  \n  // API url goes like this:\n  // https://api.particle.io/v1/devices/<deviceid>/\n  // led?args=on&access_token=<accesstoken>\n  var device_id = PropertiesService.getScriptProperties().getProperty('PARTICLE_DEVICE_ID');\n  var access_token = PropertiesService.getScriptProperties().getProperty('PARTICLE_ACCESS_TOKEN');\n  var function_name = sheet.getRange(\"B4\").getValue();\n  var request = sheet.getRange(\"B2\").getValue();\n  var url = \"https://api.particle.io/v1/devices/\" + device_id + \"/\" + function_name + \"?\" + \n    \"access_token=\" + access_token;\n\n  // Create a form which will be sent as the payload to the post request\n  var formData = {\n    'args' : request\n    \n  };\n  \n  // Create the options\n  var options =\n      {\n        \"method\"  : \"POST\", \n        \"payload\"  : formData,\n        \"followRedirects\" : true,\n        \"muteHttpExceptions\": false\n      };\n  \n  // Send Post request body to the URL indicated\n  var result = UrlFetchApp.fetch(url, options);\n  \n  // Display response in Cell\n  if (result.getResponseCode() == 200) {  \n    var params = JSON.parse(result.getContentText());\n    sheet.getRange(\"B3\").setValue(params);\n  }\n}\n```\n\nThe Get request is even simpler, as now you don't have to include a form as part of the package to be delivered. Don't forget to assign the function to the \"MAKE GET REQUEST\" button.\n\n```javascript\n// Handle the button click for Get Request -------------------------------------------------\nfunction GetRequest() {\n  // Start by trying in postman\n  \n  // Set which spreadsheet to pull or output information\n  var spreadsheet_id = SpreadsheetApp.getActiveSpreadsheet().getId();\n  var sheet = SpreadsheetApp.openById(spreadsheet_id).getSheetByName( \n    PropertiesService.getScriptProperties().getProperty('SHEET_NAME'));\n  \n  // API url goes like this: \n  // https://api.particle.io/v1/devices/<PARTICLE_DEVICE_ID>/<VARIABLE_NAME>?access_token=<PARTICLE_ACCESS_TOKEN>\n  var device_id = PropertiesService.getScriptProperties().getProperty('PARTICLE_DEVICE_ID');\n  var access_token = PropertiesService.getScriptProperties().getProperty('PARTICLE_ACCESS_TOKEN');\n  var variable_name = sheet.getRange(\"C2\").getValue();\n  var url = \"https://api.particle.io/v1/devices/\" + device_id + \"/\" + variable_name + \"?access_token=\" + access_token\n  \n  // Not sure if all options are necessary\n  var options =\n      {\n        \"method\"  : \"GET\",   \n        \"followRedirects\" : true,\n        \"muteHttpExceptions\": true\n      };\n  \n  // Send Get request body to the URL indicated\n  var result = UrlFetchApp.fetch(url, options);\n  \n  // Display response in Cell\n  if (result.getResponseCode() == 200) {\n    var params = JSON.parse(result.getContentText());\n    sheet.getRange(\"C3\").setValue(params);\n  };\n}\n```\n\nFinally, the Webhook. We need to set up the Google Sheet to be able to receive a message that is published to it. This is done through a function called `doPost()`. You will not be able to use any name you want, because doPost will be the handler to a post made externally.\n\n```javascript\n// Handle the webhook using doPost() -------------------------------------------------\nfunction doPost(e){\n  // Set which spreadsheet to output this data to\n  var spreadsheet_id = PropertiesService.getScriptProperties().getProperty('SPREADSHEET_ID');\n  var sheet = SpreadsheetApp.openById(spreadsheet_id).getSheetByName( \n    PropertiesService.getScriptProperties().getProperty('SHEET_NAME'));\n  \n  // Fill the \"Webhook Data\" cell with the parameters of the webhook package sent\n  sheet.getRange(\"D2\").setValue(e.parameter);\n  \n  return ContentService.createTextOutput(JSON.stringify({text:\"webhook received\"})).setMimeType(ContentService.MimeType.JSON);;\n}\n```\nOnce the full script is complete (full script available here, you will need to edit the Global variables section at the top: https://github.com/simjxu/partiscripts/blob/master/getpostwebhook-example/googlesheets_pgw.gs)go to your Google Apps script (https://script.google.com...), and select Publish >> Deploy as webapp. Copy the webapp URL, which you will need to setup the webhook on the Particle console. Then in the section that says \"Who has access to the app\", you will need to select \"Anyone, even anonymous\". This will ensure that the Particle console can make changes to the google sheet.\nThe webhook must also be set up as an integration on console.particle.io. with a Web Form setup. \n\n![webhook-console](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/webhooksetupconsole.jpg)\n\nUnder the Full URL section, you will need to include the target URL for your google sheets webapp. It should begin with https://script.google.com/... This is the URL you copied earlier after deploying as a webapp.\n\nThat's it! Now you have a Google sheet with a webapp that will communicate between the sheet and your Particle IoT device. Try changing the Post Request section (B2) to \"on\" or \"off\" and clicking the \"MAKE POST REQUEST\" button. Try changing cell C2 to brightness, or setpoint, and clicking \"MAKE GET REQUEST\" to view the values. Try holding your hand over the photoresistor on the photoresistor to block out enough light to trigger the webhook event.\n"}}},{"node":{"id":"87f1932e-ed55-538a-afd0-2f99b81d4af8","frontmatter":{"title":"How to use GraphQL to choose what shows up on your page","date":"14 Oct, 2018"},"fields":{"slug":"/posts/graphql_filter/"},"excerpt":"Yes, I made the switch to an uglier but more basic home webpage. If you want to make the switch too you can follow the last post I made. The…","internal":{"content":"\nYes, I made the switch to an uglier but more basic home webpage. If you want to make the switch too you can follow the last post I made. The nice thing about Gatsby is that it is easy to understand, and the \n\nThe way I currently have my site set up, I have 3 main pages: first the homepage, which is on `index.js`, an `about.js` page, and a `contact.js` page. I may switch this later to have about and contact on the same page, and have a projects page instead.\n\nOn the `index.js` page, there is a GraphQL query, which filters what gets displayed on my index page. First, I look for all markdown files (files in my directory that are .md), and filter and sort it according layout and date, respectively. You can see that when I do `filter: { frontmatter: {layout: {eq: \"post\"}}}`, it uses a Sift syntax, which is used in MongoDB, to filter for all markdown files that have a layout entry in the frontmatter that is labeled \"post\". This is because I also have markdown files for my about, and contact pages, but I want to ensure only the ones that have \"post\" in the front matter get displayed. \n\nOne of the most useful things that comes with Gatsby when you do `gatsby develop` is that you get an in browser IDE called GraphiQL (prounounced \"graphical\", cute). This allows you to make all the graphQL filters you want, and see what it outputs. Super convenient, super easy. I love using Ctrl+Space in order to see what options I have available.\n\nIn order to publish this on Github pages you have to add a dependency, called gh-pages amd add at deploy script `\"deploy\": \"gatsby build && gh-pages -d public -b master\",` to your package.json file.\n\nUpdate 11/4/18:\nThere was a bug similar to the one shown here: https://github.com/gatsbyjs/gatsby/issues/5734\nI rebuilt the website and as you can see, no longer any issue, although I'm not sure why. I didn't include Typography this time, but I suspect there was some sort of CSS in JS plugin that was not functioning properly."}}},{"node":{"id":"7084c808-5b01-56d9-85cf-863e222819f6","frontmatter":{"title":"How GraphQL is used in Gatsby","date":"30 Sep, 2018"},"fields":{"slug":"/posts/gatsby1/"},"excerpt":"I'm currently in the process of converting this blog into a Gatsby blog, and I've spent a full day trying to get it working. Thankfully…","internal":{"content":"\nI'm currently in the process of converting this blog into a Gatsby blog, and I've spent a full day trying to get it working. Thankfully, they do seem to have good documentation on their website <a href=\"https://www.gatsbyjs.org/tutorial/\">Gatsby Tutorial</a>. The going is quite slow, and the results are also quite ugly, but after trying to figure out GraphQL and aggravating myself over backticks, at least I have something.\n![gatsby-1](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/gatsby1.jpg)\n\nI'll also push the gatsby version of this site onto github here: https://github.com/simjxu/gatsby-themush. You're probably wondering, what's the point of pushing something so useless onto github?  Well, my plan is to continue iterating on this Gatsby blog, and I also intend to litter the darn thing with comments. Hopefully this will help others understand the nuances of building a site. I don't have many comments yet, but stay tuned and I expect there to be more commentary than there is code.\n\nThe tutorial does a good job at helping you understand how GraphQL allows different page components to query particular items. One of the first uses of GraphQL is to swap out any mention of the website title with a `{data.site.siteMetadata.title}`. This does a page query to another file that must be titled gatsby-config.js to determine the website's metadata, like its title. This means that anytime a title is referenced, it queries the metadata. If the website title were to change at any point, then they can all be changed because of the query.\n\nAnother place where queries are used are for the blog posts. You can set GraphQL to automatically filter out any markdown file within your \"pages\" folder on Gatsby and list them on the homepage of the website. This is any file that ends in .md. To create that query that can point to the blogposts, you need to create a path to the location, and for some reason they call this a \"slug\". The page can then be created by using two Gatsby APIs, `onCreateNode` and `createPages`. \n\nTo be continued, SOON."}}},{"node":{"id":"3305f733-cc16-59f2-bd3d-eb1c2493426d","frontmatter":{"title":"How to run a Python Script on Google Compute Engine","date":"19 Sep, 2018"},"fields":{"slug":"/posts/googlecomputeengine/"},"excerpt":"Probably the most familiar experience of a hardware startup is the lack of monetary resources (free lunches? no way!). And yet, we still…","internal":{"content":"\nProbably the most familiar experience of a hardware startup is the lack of monetary resources (free lunches? no way!). And yet, we still want to be cutting edge with our machine learning techniques. Sometimes, doing a custom feature extraction from our datasets can take a lot of computing resources, taking up to 8 hours on a laptop running Linux (notice how I did not say a Macbook Pro). Thankfully, Big Brother Google wants to trickle down some benefits for the needy, and provides us with a Free Tier for a lot of there cloud platform modules. So, I decided to try this out.\n\nFirst, I just wrote a simple Python program (testscript.py) that takes more than just a second to run, by doing the forbidden: nested for loops, the Avada Kedavra of programming.\n\n~~~ python\nnumruns = 300\nprint(\"running\")\nhuge_array = [[[0.0 for i in range(numruns)] for j in range(numruns)] for k in range(numruns)]\ncounter = 0\nprint(\"starting loops\")\nfor i in range(numruns):\n    for j in range(numruns):\n        for k in range(numruns):\n            huge_array[i][j][k] = i+j+k\n            counter+=1\n            if counter%10000 == 0:\n                print(counter)\nprint(\"finished\")\n~~~\n\nThe first thing to do is to create a project on GCP (Google Cloud Platform) by going to console.cloud.google.com and clicking on \"Select a Project\". Once you have named your project, notice you can take advantage of $300 of free credits to play with (how generous!). Once you have made your project, you can click on \"Compute Engine\" on the left hand panel, and create an instance. As of the time this post was written, you can get a free f1-micro instance running on a virtual machine in the US. This f1-micro has 0.6GB of memory and 1vCPU.\n![gce-f1micro](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/gce-dashboard.jpg)\n\nAfter this instance is created, you can view your instance and also SSH into the instance. If you do this, then you can interact with this instance via a Debian Linux shell. To upload a file, you can click on the gearbox on the top right side to upload your python file. In my case, I am uploading a python 3 script called testscript.py, which I shared above. \n![gce-f1microShell](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/f1micro-SSHshell.jpg)\n\nAfter it is uploaded, you can run the file on the instance by first making it an executable with `chmod +x testscript.py`. `chmod` basically changes the mode and using the `+x` option makes it an executable. Then you can run `nohup` on the python script to get it running in the shell without requiring the shell to be open. Unfortunately, with the python script I shared, it gets killed after a few seconds:\n![gce-f1microKilled](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/f1micro-killed.jpg)\n\nTurns out free also means you can't just leave something running forever. If you check the nohup.out file using `cat nohup.out`, you can see that my script ended somewhere after iteration 523000. Probably exceeded the RAM available here. Probably the better thing to do is to take advantage of another free item, the 5GB of cloud storage. I'll try that next time.\n\nUseful references: https://cloud.google.com/free/docs/always-free-usage-limits\n"}}},{"node":{"id":"c5806368-9a38-5f7c-9961-43f4bf5ef877","frontmatter":{"title":"How to use Slack to Post to a Google Sheets Database","date":"17 Sep, 2018"},"fields":{"slug":"/posts/slackdeliverytracker/"},"excerpt":"Since I work for an IIoT combined hardware/sofware company, we often receive packages through the mail. This includes anything from dev…","internal":{"content":"\nSince I work for an IIoT combined hardware/sofware company, we often receive packages through the mail. This includes anything from dev boards to etched plastic enclosures. Now that we receive packages so often, it is not very easy to keep track of when a package is supposed to be arriving, and we don't always get notifications when items get held up in customs or get lost in the process. Once again APIs are here to save the day.\n\nIn the ideal scenario, we could create a Chrome extension with a trained machine learning algorithm that can detect whether a purchase has been made for a delivery, guess the delivery timeframe, store the delivery in a database, and automatically alert the user if today's date is later than the anticipated delivery date. However, even this scenario doesn't account for company purchases made with a purchase order system, which is why procurement teams exist.\n\nGiven that I am no genius, an idiot's backup to the system above is to use a slack slash command to automatically update a simple database, like a spreadsheet in Google Sheets for example. Then, using an incoming webhook, send an alert if it is past the delivery date for the package, triggered once every day to a channel, perhaps called #deliveryalerts. If there are no packages that are supposed to be delivered already, no alerts. \n\nWe can first create a simple spreadsheet database like this: \n![vendor-spreadsheet](https://raw.githubusercontent.com/simjxu/simjxu.github.io/gh-pages/img/vendor_spreadsheet.jpg)\nThis would show the item that is being delivered, the estimated delivery date, and a boolean Yes/No as to whether the item has been received.\n\nThen, we can create the #deliveryalerts channel in Slack, and integrate a Slack incoming webhook that points to #deliveryalerts and a slash command. In this case, we create one called /delivery. /delivery will send a post request to some endpoint that you need to define (a url). In our case, we can just use Google Apps script to set up this endpoint. Anything written after the /delivery command will be sent to the endpoint, and we can create the script to parse this text message using regular expressions (regex). Google Apps Script will handle the request through a `doPost()` function that you must write. Just take the spreadsheet that you want to set as your deliveries database, and on the Tools menu select script editor. Copy the `doPost()` example below into the Code.gs file.\n\n```javascript\n// Handle the Post request, based upon the command that is written after the slash command\nfunction doPost(e){\n  var commandReceived = e.parameter[\"text\"];\n\n  if (commandReceived.match(/help/)) showHelp();\t\t\t// showHelp() is run when slack user types: /delivery help\n  if (commandReceived.match(/list/)) listDeliveries();\n  if (commandReceived.match(/add/)) add(e);\n  if (commandReceived.match(/remove/)) remove(e);\t\t\t// remove(e) is run after slack user types: /delivery remove someItem\n  if (commandReceived.match(/received/)) received(e);\n  \n  // Need to create a return, otherwise slack will complain that there was no response created\n  var returnMessage = \"send complete\";\n  return ContentService.createTextOutput(JSON.stringify({text:returnMessage})).setMimeType(ContentService.MimeType.JSON);\n}\n```\n\nSlack complains if there is no return message, so I've also included just a simple \"send complete\" at the end of the `doPost()` function. Now, all you need to do is create the functions to handle each of the if statements in `doPost()`. An example of this function can be seen here, for marking a delivery as received. Notice that regular expressions are used to determine the format for the string that is attached to the slash command.\n\n~~~ javascript\nfunction received(e){\n  var receiver = e.parameter[\"user_name\"];\n  var messageReceived = e.parameter[\"text\"].trim();\n  var regex = /received ([a-zA-Z0-9-_\\s]+)/;\n  var matches = regex.exec(messageReceived);\n  var deliveryName = matches[1];\n  var sheet = getStatusSheet();\n  var affectedRow = getDeliveryRow(deliveryName);\n  \n  if (affectedRow) {\n    sheet.getRange(\"D\" + (affectedRow)).setValue('Y');\n    getLogger().log(\"%s received delivery %s\", receiver, deliveryName);\n    listDeliveries();\n  } else {\n    sendMessage(\"*\" + deliveryName + \"* delivery not found\");\n  }\n}\n~~~\n\nThen there's the timed triggers. To do this, I used a trigger function, which runs another function I made, querySpreadsheet, once every day. I also created a button on a html page to turn on/off this trigger.\n~~~ javascript\nfunction createTrigger() {\n  ScriptApp.newTrigger('querySpreadsheet')\n  .timeBased()\n  .everyDays(1)\n  .create()\n}\n~~~\n\nAnother note, I used BetterLog to create another spreadsheet tab that keeps track of the different actions users take with the app. This helps us know who received what package in case we need to track one down. You'll have to add the BetterLog library to your project.\n\nTo set everything up on the Slack end, login to <a href=\"https://api.slack.com/apps\">api.slack.com/apps</a>, create a new app, and Add features and functionality, adding Incoming Webhooks, Interactive Components, and Slash Commands. You will need to copy the incoming webhooks link into the Google Apps Script. \n\nThe full script is located in this github repository: <a href=\"https://github.com/simjxu/google_apps_scripts/tree/master/Slack-Delivery-Tracking\">Slack Delivery Tracking</a>  Be sure to change out the spreadsheet id to match the spreadsheet id of your Google sheets document (the long string in your Google Sheet URL), and change the incoming webhook link to the webhook link that you receive when you enable that feature on your slack apps. You might want to test by changing the `createTrigger()` function to be `.everyMinutes(1)` instead of `.everyDays(1)`.\n\nTo Do: to reduce time that it takes to read through the excel sheet, it's best to add within the trigger function something that places the received packages into some sort of archive spreadsheet tab. But alas, I already took too long to create this post. I can't take this long if I want to see rows of green squares on my github.\n\nUseful references: https://medium.com/@hopsor/building-a-slack-serverless-bot-with-google-apps-script-and-spreadsheets-35bdac755a44\n"}}},{"node":{"id":"c0b8af4b-c913-5a6c-9bc8-c245ea018dd4","frontmatter":{"title":"Flask and Django - Python options","date":"10 Sep, 2018"},"fields":{"slug":"/posts/mfgapp/"},"excerpt":"The company I work for makes internet connected hardware for collecting machine data. So yes, this involves actually building a physical…","internal":{"content":"\nThe company I work for makes internet connected hardware for collecting machine data. So yes, this involves actually building a physical product that has to be constructed from solder, glue, and rhino tears. Because of this, we have to also build a software application that is able to test functionality of the device: a manufacturing webapp that runs on the cloud, collects functional test data, and stores serial numbers/calibration values on a Postgres database. Since this web application is not customer facing, it gets all the care, love, and attention of a donation request letter in your snail mail. \n\nAt the time this was built, the developers had experience developing in Python, and they chose to develop the frontend in Angular using Flask. Why they chose Flask over Django eludes me, but upon doing some research it seems like Flask has more of a minimalist/bare bones, add-functionality-as-you-need-it type of approach, while Django comes with the bells and whistles included. \n\nHere's everything I know about building this web application at the moment. There exists an index.html file in the app directory, and within that index.html file there are script src files which point to other angular source files which have too many lines of code for me to understand at the moment.\n\n~~~ html\n<script src=\"bower_components/jquery/dist/jquery.js\"></script>\n<script src=\"bower_components/angular/angular.js\"></script>\n<script src=\"bower_components/bootstrap/dist/js/bootstrap.js\"></script>\n~~~\n\nSince I have no real understanding of the architecture of the existing Angular/Flask web application right now, I'll go ahead and start from scratch, building the web application with React/NodeJS instead.\n\n1st step in getting familiar with React is to change this jekyll blog into a Gatsby blog: \n<a href=\"https://www.gatsbyjs.org/docs/deploy-gatsby/#github-pages\">https://www.gatsbyjs.org/docs/deploy-gatsby/#github-pages</a>\nThis static website generator uses React, Webpack, and GraphQL, all the popular kids at the pool right now. For those of you who are new to engineering: the most critical skill as an engineer is to be able to read other people's documentation."}}},{"node":{"id":"bad11a45-7ca0-5880-9fc2-96d778e7dec4","frontmatter":{"title":"Yeah I'm gonna go rock climbing","date":"09 Sep, 2018"},"fields":{"slug":"/posts/goingclimbing/"},"excerpt":"It's good to start expectations low for the first post. Commitment is hard, and you never know if a blog will actually continue far past the…","internal":{"content":"\nIt's good to start expectations low for the first post. Commitment is hard, and you never know if a blog will actually continue far past the first few posts. Anyway, this blog is intended to record me as I self-teach web development. Follow along if you're on the same page. Luckily, I'm not really starting from scratch, at this point I've been working for 8 years. 3 of those years I've spent working in IoT. I've developed tools in MATLAB, Python, LabVIEW, Squirrel/C, and also done a tiny bit of node development, mostly to create an endpoint for my Electric Imp module post requests. \n\nI used to focus primarily on hardware work, and my old blog talked nearly exclusively about digital signal processing (https://simonxu.wordpress.com/). I was also a lot less funny then. Now this will be my new blog. I'm using github pages with jekyll and I'm using a template someone else came up with (credit is on the bottom of the page).\n\nRecently, I decided that hardware is too hard, and thus begins my switch. No guarantees. I'm going rock climbing.\n\n~~~ js\nvar blog = require('persistence');\n~~~\n"}}}]}},"pageContext":{}}