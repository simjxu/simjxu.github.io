{"data":{"allMarkdownRemark":{"totalCount":6,"edges":[{"node":{"id":"65963faa-ad90-55ba-8fe8-d1bb395d1f3c","frontmatter":{"title":"How to use GraphQL to choose what shows up on your page","date":"14 Oct, 2018"},"fields":{"slug":"/posts/graphql_filter/"},"excerpt":"Yes, I made the switch to an uglier but more understandable home webpage.  The way I currently have my site set up, I have 3 main pages…","internal":{"content":"Yes, I made the switch to an uglier but more understandable home webpage. \n\nThe way I currently have my site set up, I have 3 main pages: first the homepage, which is on `index.js`, an `about.js` page, and a `contact.js` page. I may switch this later to have about and contact on the same page, and have a projects page instead.\n\nOn the `index.js` page, there is a GraphQL query, which filters what gets displayed on my index page. First, I look for all markdown files (files in my directory that are .md), and filter and sort it according layout and date, respectively. You can see that when I do `filter: { frontmatter: {layout: {eq: \"post\"}}}`, it uses a Sift syntax, which is used in MongoDB, to filter for all markdown files that have a layout entry in the frontmatter that is labeled \"post\". This is because I also have markdown files for my about, and contact pages, but I want to ensure only the ones that have \"post\" in the front matter get displayed."}}},{"node":{"id":"98e1d3bd-7c32-50c6-89ab-a61e4c6aac0b","frontmatter":{"title":"How GraphQL is used in Gatsby","date":"30 Sep, 2018"},"fields":{"slug":"/posts/2018-09-30-gatsby1/"},"excerpt":"I'm currently in the process of converting this blog into a Gatsby blog, and I've spent a full day trying to get it working. Thankfully…","internal":{"content":"\nI'm currently in the process of converting this blog into a Gatsby blog, and I've spent a full day trying to get it working. Thankfully, they do seem to have good documentation on their website <a href=\"https://www.gatsbyjs.org/tutorial/\">Gatsby Tutorial</a>. The going is quite slow, and the results are also quite ugly, but after trying to figure out GraphQL and aggravating myself over backticks, at least I have something.\n![gatsby-1](https://raw.githubusercontent.com/simjxu/simjxu.github.io/master/img/gatsby1.jpg)\n\nI'll also push the gatsby version of this site onto github here: https://github.com/simjxu/gatsby-themush. You're probably wondering, what's the point of pushing something so useless onto github?  Well, my plan is to continue iterating on this Gatsby blog, and I also intend to litter the darn thing with comments. Hopefully this will help others understand the nuances of building a site. I don't have many comments yet, but stay tuned and I expect there to be more commentary than there is code.\n\nThe tutorial does a good job at helping you understand how GraphQL allows different page components to query particular items. One of the first uses of GraphQL is to swap out any mention of the website title with a `{data.site.siteMetadata.title}`. This does a page query to another file that must be titled gatsby-config.js to determine the website's metadata, like its title. This means that anytime a title is referenced, it queries the metadata. If the website title were to change at any point, then they can all be changed because of the query.\n\nAnother place where queries are used are for the blog posts. You can set GraphQL to automatically filter out any markdown file within your \"pages\" folder on Gatsby and list them on the homepage of the website. This is any file that ends in .md. To create that query that can point to the blogposts, you need to create a path to the location, and for some reason they call this a \"slug\". The page can then be created by using two Gatsby APIs, `onCreateNode` and `createPages`. \n\nTo be continued.\n"}}},{"node":{"id":"f9c168c6-1257-5b9b-ae49-5d85d762c345","frontmatter":{"title":"How to run a Python Script on Google Compute Engine","date":"19 Sep, 2018"},"fields":{"slug":"/posts/googlecomputeengine/"},"excerpt":"Probably the most familiar experience of a hardware startup is the lack of monetary resources (free lunches? no way!). And yet, we still…","internal":{"content":"\nProbably the most familiar experience of a hardware startup is the lack of monetary resources (free lunches? no way!). And yet, we still want to be cutting edge with our machine learning techniques. Sometimes, doing a custom feature extraction from our datasets can take a lot of computing resources, taking up to 8 hours on a laptop running Linux (notice how I did not say a Macbook Pro). Thankfully, Big Brother Google wants to trickle down some benefits for the needy, and provides us with a Free Tier for a lot of there cloud platform modules. So, I decided to try this out.\n\nFirst, I just wrote a simple Python program (testscript.py) that takes more than just a second to run, by doing the forbidden: nested for loops, the Avada Kedavra of programming.\n\n~~~ python\nnumruns = 300\nprint(\"running\")\nhuge_array = [[[0.0 for i in range(numruns)] for j in range(numruns)] for k in range(numruns)]\ncounter = 0\nprint(\"starting loops\")\nfor i in range(numruns):\n    for j in range(numruns):\n        for k in range(numruns):\n            huge_array[i][j][k] = i+j+k\n            counter+=1\n            if counter%10000 == 0:\n                print(counter)\nprint(\"finished\")\n~~~\n\nThe first thing to do is to create a project on GCP (Google Cloud Platform) by going to console.cloud.google.com and clicking on \"Select a Project\". Once you have named your project, notice you can take advantage of $300 of free credits to play with (how generous!). Once you have made your project, you can click on \"Compute Engine\" on the left hand panel, and create an instance. As of the time this post was written, you can get a free f1-micro instance running on a virtual machine in the US. This f1-micro has 0.6GB of memory and 1vCPU.\n![gce-f1micro](https://raw.githubusercontent.com/simjxu/simjxu.github.io/master/img/gce-dashboard.jpg)\n\nAfter this instance is created, you can view your instance and also SSH into the instance. If you do this, then you can interact with this instance via a Debian Linux shell. To upload a file, you can click on the gearbox on the top right side to upload your python file. In my case, I am uploading a python 3 script called testscript.py, which I shared above. \n![gce-f1microShell](https://raw.githubusercontent.com/simjxu/simjxu.github.io/master/img/f1micro-SSHshell.jpg)\n\nAfter it is uploaded, you can run the file on the instance by first making it an executable with `chmod +x testscript.py`. `chmod` basically changes the mode and using the `+x` option makes it an executable. Then you can run `nohup` on the python script to get it running in the shell without requiring the shell to be open. Unfortunately, with the python script I shared, it gets killed after a few seconds:\n![gce-f1microKilled](https://raw.githubusercontent.com/simjxu/simjxu.github.io/master/img/f1micro-killed.jpg)\n\nTurns out free also means you can't just leave something running forever. If you check the nohup.out file using `cat nohup.out`, you can see that my script ended somewhere after iteration 523000. Probably exceeded the RAM available here. Probably the better thing to do is to take advantage of another free item, the 5GB of cloud storage. I'll try that next time.\n\nUseful references: https://cloud.google.com/free/docs/always-free-usage-limits\n"}}},{"node":{"id":"5b7f9e5c-d310-5f8d-b38e-6a7d084a1058","frontmatter":{"title":"How to use Slack to Post to a Google Sheets Database","date":"17 Sep, 2018"},"fields":{"slug":"/posts/slackdeliverytracker/"},"excerpt":"Since I work for an IIoT combined hardware/sofware company, we often receive packages through the mail. This includes anything from dev…","internal":{"content":"\nSince I work for an IIoT combined hardware/sofware company, we often receive packages through the mail. This includes anything from dev boards to etched plastic enclosures. Now that we receive packages so often, it is not very easy to keep track of when a package is supposed to be arriving, and we don't always get notifications when items get held up in customs or get lost in the process. Once again APIs are here to save the day.\n\nIn the ideal scenario, we could create a Chrome extension with a trained machine learning algorithm that can detect whether a purchase has been made for a delivery, guess the delivery timeframe, store the delivery in a database, and automatically alert the user if today's date is later than the anticipated delivery date. However, even this scenario doesn't account for company purchases made with a purchase order system, which is why procurement teams exist.\n\nGiven that I am no genius, an idiot's backup to the system above is to use a slack slash command to automatically update a simple database, like a spreadsheet in Google Sheets for example. Then, using an incoming webhook, send an alert if it is past the delivery date for the package, triggered once every day to a channel, perhaps called #deliveryalerts. If there are no packages that are supposed to be delivered already, no alerts. \n\nWe can first create a simple spreadsheet database like this: \n![vendor-spreadsheet](https://raw.githubusercontent.com/simjxu/simjxu.github.io/master/img/vendor_spreadsheet.jpg)\nThis would show the item that is being delivered, the estimated delivery date, and a boolean Yes/No as to whether the item has been received.\n\nThen, we can create the #deliveryalerts channel in Slack, and integrate a Slack incoming webhook that points to #deliveryalerts and a slash command. In this case, we create one called /delivery. /delivery will send a post request to some endpoint that you need to define (a url). In our case, we can just use Google Apps script to set up this endpoint. Anything written after the /delivery command will be sent to the endpoint, and we can create the script to parse this text message using regular expressions (regex). Google Apps Script will handle the request through a `doPost()` function that you must write. Just take the spreadsheet that you want to set as your deliveries database, and on the Tools menu select script editor. Copy the `doPost()` example below into the Code.gs file.\n\n```javascript\n// Handle the Post request, based upon the command that is written after the slash command\nfunction doPost(e){\n  var commandReceived = e.parameter[\"text\"];\n\n  if (commandReceived.match(/help/)) showHelp();\t\t\t// showHelp() is run when slack user types: /delivery help\n  if (commandReceived.match(/list/)) listDeliveries();\n  if (commandReceived.match(/add/)) add(e);\n  if (commandReceived.match(/remove/)) remove(e);\t\t\t// remove(e) is run after slack user types: /delivery remove someItem\n  if (commandReceived.match(/received/)) received(e);\n  \n  // Need to create a return, otherwise slack will complain that there was no response created\n  var returnMessage = \"send complete\";\n  return ContentService.createTextOutput(JSON.stringify({text:returnMessage})).setMimeType(ContentService.MimeType.JSON);\n}\n```\n\nSlack complains if there is no return message, so I've also included just a simple \"send complete\" at the end of the `doPost()` function. Now, all you need to do is create the functions to handle each of the if statements in `doPost()`. An example of this function can be seen here, for marking a delivery as received. Notice that regular expressions are used to determine the format for the string that is attached to the slash command.\n\n~~~ javascript\nfunction received(e){\n  var receiver = e.parameter[\"user_name\"];\n  var messageReceived = e.parameter[\"text\"].trim();\n  var regex = /received ([a-zA-Z0-9-_\\s]+)/;\n  var matches = regex.exec(messageReceived);\n  var deliveryName = matches[1];\n  var sheet = getStatusSheet();\n  var affectedRow = getDeliveryRow(deliveryName);\n  \n  if (affectedRow) {\n    sheet.getRange(\"D\" + (affectedRow)).setValue('Y');\n    getLogger().log(\"%s received delivery %s\", receiver, deliveryName);\n    listDeliveries();\n  } else {\n    sendMessage(\"*\" + deliveryName + \"* delivery not found\");\n  }\n}\n~~~\n\nThen there's the timed triggers. To do this, I used a trigger function, which runs another function I made, querySpreadsheet, once every day. I also created a button on a html page to turn on/off this trigger.\n~~~ javascript\nfunction createTrigger() {\n  ScriptApp.newTrigger('querySpreadsheet')\n  .timeBased()\n  .everyDays(1)\n  .create()\n}\n~~~\n\nAnother note, I used BetterLog to create another spreadsheet tab that keeps track of the different actions users take with the app. This helps us know who received what package in case we need to track one down. You'll have to add the BetterLog library to your project.\n\nTo set everything up on the Slack end, login to <a href=\"https://api.slack.com/apps\">api.slack.com/apps</a>, create a new app, and Add features and functionality, adding Incoming Webhooks, Interactive Components, and Slash Commands. You will need to copy the incoming webhooks link into the Google Apps Script. \n\nThe full script is located in this github repository: <a href=\"https://github.com/simjxu/google_apps_scripts/tree/master/Slack-Delivery-Tracking\">Slack Delivery Tracking</a>  Be sure to change out the spreadsheet id to match the spreadsheet id of your Google sheets document (the long string in your Google Sheet URL), and change the incoming webhook link to the webhook link that you receive when you enable that feature on your slack apps. You might want to test by changing the `createTrigger()` function to be `.everyMinutes(1)` instead of `.everyDays(1)`.\n\nTo Do: to reduce time that it takes to read through the excel sheet, it's best to add within the trigger function something that places the received packages into some sort of archive spreadsheet tab. But alas, I already took too long to create this post. I can't take this long if I want to see rows of green squares on my github.\n\nUseful references: https://medium.com/@hopsor/building-a-slack-serverless-bot-with-google-apps-script-and-spreadsheets-35bdac755a44\n"}}},{"node":{"id":"830400f9-c36e-549d-be2d-5a45383becc1","frontmatter":{"title":"Flask and Django - Python options","date":"10 Sep, 2018"},"fields":{"slug":"/posts/mfgapp/"},"excerpt":"The company I work for makes internet connected hardware for collecting machine data. So yes, this involves actually building a physical…","internal":{"content":"\nThe company I work for makes internet connected hardware for collecting machine data. So yes, this involves actually building a physical product that has to be constructed from solder, glue, and rhino tears. Because of this, we have to also build a software application that is able to test functionality of the device: a manufacturing webapp that runs on the cloud, collects functional test data, and stores serial numbers/calibration values on a Postgres database. Since this web application is not customer facing, it gets all the care, love, and attention of a donation request letter in your snail mail. \n\nAt the time this was built, the developers had experience developing in Python, and they chose to develop the frontend in Angular using Flask. Why they chose Flask over Django eludes me, but upon doing some research it seems like Flask has more of a minimalist/bare bones, add-functionality-as-you-need-it type of approach, while Django comes with the bells and whistles included. \n\nHere's everything I know about building this web application at the moment. There exists an index.html file in the app directory, and within that index.html file there are script src files which point to other angular source files which have too many lines of code for me to understand at the moment.\n\n~~~ html\n<script src=\"bower_components/jquery/dist/jquery.js\"></script>\n<script src=\"bower_components/angular/angular.js\"></script>\n<script src=\"bower_components/bootstrap/dist/js/bootstrap.js\"></script>\n~~~\n\nSince I have no real understanding of the architecture of the existing Angular/Flask web application right now, I'll go ahead and start from scratch, building the web application with React/NodeJS instead.\n\n1st step in getting familiar with React is to change this jekyll blog into a Gatsby blog: \n<a href=\"https://www.gatsbyjs.org/docs/deploy-gatsby/#github-pages\">https://www.gatsbyjs.org/docs/deploy-gatsby/#github-pages</a>\nThis static website generator uses React, Webpack, and GraphQL, all the popular kids at the pool right now. For those of you who are new to engineering: the most critical skill as an engineer is to be able to read other people's documentation."}}},{"node":{"id":"e88ce11d-ab27-5cf0-86e3-9959a8481b98","frontmatter":{"title":"Yeah I'm gonna go rock climbing","date":"09 Sep, 2018"},"fields":{"slug":"/posts/goingclimbing/"},"excerpt":"It's good to start expectations low for the first post. Commitment is hard, and you never know if a blog will actually continue far past the…","internal":{"content":"\nIt's good to start expectations low for the first post. Commitment is hard, and you never know if a blog will actually continue far past the first few posts. Anyway, this blog is intended to record me as I self-teach web development. Follow along if you're on the same page. Luckily, I'm not really starting from scratch, at this point I've been working for 8 years. 3 of those years I've spent working in IoT. I've developed tools in MATLAB, Python, LabVIEW, Squirrel/C, and also done a tiny bit of node development, mostly to create an endpoint for my Electric Imp module post requests. \n\nI used to focus primarily on hardware work, and my old blog talked nearly exclusively about digital signal processing (https://simonxu.wordpress.com/). I was also a lot less funny then. Now this will be my new blog. I'm using github pages with jekyll and I'm using a template someone else came up with (credit is on the bottom of the page).\n\nRecently, I decided that hardware is too hard, and thus begins my switch. No guarantees. I'm going rock climbing.\n\n~~~ js\nvar blog = require('persistence');\n~~~\n"}}}]}},"pageContext":{}}